{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/.conda/envs/jax311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "from basin_volume import VolumeConfig, VolumeEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basin_volume import ImplicitParamVector, ImplicitRandomVector, ImplicitVector\n",
    "from basin_volume import print_gpu_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-14m\")\n",
    "model.cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-14m\")\n",
    "tokenizer.pad_token_id = 1  # pythia-specific\n",
    "tokenizer.eos_token_id = 0  # pythia-specific\n",
    "dataset = load_dataset(\"EleutherAI/lambada_openai\", name=\"en\", split=\"test\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens.shape=torch.Size([52947, 8])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg = VolumeConfig(model=model, \n",
    "                   tokenizer=tokenizer, \n",
    "                   dataset=dataset, \n",
    "                   text_key=\"text\",  # must match dataset field\n",
    "                   n_samples=10,  # number of MC samples\n",
    "                   cutoff=1e-2,  # KL-divergence cutoff (nats)\n",
    "                   max_seq_len=8,  # sequence length for chunking dataset\n",
    "                   val_size=10,  # number of sequences or chunks to use in estimation\n",
    "                   data_batch_size=1,\n",
    "                   cache_mode=None,\n",
    "                   chunking=True,\n",
    "                   implicit_vectors=False,\n",
    "                   debug=False,\n",
    "                   )\n",
    "estimator = VolumeEstimator.from_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after randn\n",
      "\n",
      "Current GPU memory allocated: 85.80 MB\n",
      "Max GPU memory allocated: 85.80 MB\n",
      "Current GPU memory reserved: 98.00 MB\n",
      "Max GPU memory reserved: 98.00 MB\n",
      "\n",
      "Current GPU memory allocated: 85.80 MB\n",
      "Max GPU memory allocated: 101.80 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:16<02:26, 16.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current GPU memory allocated: 139.47 MB\n",
      "Max GPU memory allocated: 139.47 MB\n",
      "Current GPU memory reserved: 164.00 MB\n",
      "Max GPU memory reserved: 164.00 MB\n",
      "after randn\n",
      "\n",
      "Current GPU memory allocated: 139.49 MB\n",
      "Max GPU memory allocated: 139.52 MB\n",
      "Current GPU memory reserved: 164.00 MB\n",
      "Max GPU memory reserved: 164.00 MB\n",
      "\n",
      "Current GPU memory allocated: 139.49 MB\n",
      "Max GPU memory allocated: 155.49 MB\n",
      "Current GPU memory reserved: 164.00 MB\n",
      "Max GPU memory reserved: 164.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:32<02:09, 16.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current GPU memory allocated: 193.16 MB\n",
      "Max GPU memory allocated: 193.16 MB\n",
      "Current GPU memory reserved: 208.00 MB\n",
      "Max GPU memory reserved: 208.00 MB\n",
      "after randn\n",
      "\n",
      "Current GPU memory allocated: 193.19 MB\n",
      "Max GPU memory allocated: 193.22 MB\n",
      "Current GPU memory reserved: 208.00 MB\n",
      "Max GPU memory reserved: 208.00 MB\n",
      "\n",
      "Current GPU memory allocated: 193.19 MB\n",
      "Max GPU memory allocated: 209.19 MB\n",
      "Current GPU memory reserved: 230.00 MB\n",
      "Max GPU memory reserved: 230.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:49<01:56, 16.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current GPU memory allocated: 246.86 MB\n",
      "Max GPU memory allocated: 246.86 MB\n",
      "Current GPU memory reserved: 254.00 MB\n",
      "Max GPU memory reserved: 254.00 MB\n",
      "after randn\n",
      "\n",
      "Current GPU memory allocated: 246.88 MB\n",
      "Max GPU memory allocated: 246.91 MB\n",
      "Current GPU memory reserved: 254.00 MB\n",
      "Max GPU memory reserved: 254.00 MB\n",
      "\n",
      "Current GPU memory allocated: 246.88 MB\n",
      "Max GPU memory allocated: 262.88 MB\n",
      "Current GPU memory reserved: 276.00 MB\n",
      "Max GPU memory reserved: 276.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:57<02:13, 19.03s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/ssd-1/adam/basin-volume/notebooks/huggingface_example.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Badam-ord/mnt/ssd-1/adam/basin-volume/notebooks/huggingface_example.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m result \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m/mnt/ssd-1/adam/basin-volume/src/basin_volume/estimator.py:115\u001b[0m, in \u001b[0;36mVolumeEstimator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdebug:\n\u001b[1;32m    113\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39msigma\u001b[39m \u001b[39m\u001b[39m= }\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 115\u001b[0m \u001b[39mreturn\u001b[39;00m get_estimates_vectorized_gauss(\n\u001b[1;32m    116\u001b[0m     n\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mn_samples,\n\u001b[1;32m    117\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mmodel_batch_size,\n\u001b[1;32m    118\u001b[0m     sigma\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49msigma,\n\u001b[1;32m    119\u001b[0m     preconditioner\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreconditioner,\n\u001b[1;32m    120\u001b[0m     fn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkl_fn,\n\u001b[1;32m    121\u001b[0m     params\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams,\n\u001b[1;32m    122\u001b[0m     tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mtol,\n\u001b[1;32m    123\u001b[0m     y_tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49my_tol,\n\u001b[1;32m    124\u001b[0m     seed\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mseed,\n\u001b[1;32m    125\u001b[0m     cutoff\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mcutoff,\n\u001b[1;32m    126\u001b[0m     with_tqdm\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mtqdm,\n\u001b[1;32m    127\u001b[0m     debug\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mdebug\n\u001b[1;32m    128\u001b[0m )\n",
      "File \u001b[0;32m/mnt/ssd-1/adam/basin-volume/src/basin_volume/volume.py:111\u001b[0m, in \u001b[0;36mget_estimates_vectorized_gauss\u001b[0;34m(n, sigma, batch_size, preconditioner, fn, unary_fn, params, gaussint_fn, debug, tol, y_tol, seed, with_tqdm, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m print_gpu_memory()\n\u001b[1;32m    110\u001b[0m kwargs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mcutoff\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1e-3\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfn\u001b[39m\u001b[39m'\u001b[39m: fn, \u001b[39m'\u001b[39m\u001b[39miters\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m100\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrtol\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1e-2\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[0;32m--> 111\u001b[0m mults, deltas \u001b[39m=\u001b[39m find_radius_vectorized(center, vecs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    113\u001b[0m x1 \u001b[39m=\u001b[39m mults \u001b[39m*\u001b[39m props\n\u001b[1;32m    114\u001b[0m a \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m sigma\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n",
      "File \u001b[0;32m/mnt/ssd-1/adam/basin-volume/src/basin_volume/volume.py:28\u001b[0m, in \u001b[0;36mfind_radius_vectorized\u001b[0;34m(center, vecs, cutoff, fn, rtol, init_mult, iters, jump)\u001b[0m\n\u001b[1;32m     25\u001b[0m deltas \u001b[39m=\u001b[39m vec_losses \u001b[39m-\u001b[39m center_losses\n\u001b[1;32m     27\u001b[0m \u001b[39mwhile\u001b[39;00m iters \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39mabs\u001b[39m(deltas \u001b[39m-\u001b[39m cutoff) \u001b[39m>\u001b[39m cutoff \u001b[39m*\u001b[39m rtol):\n\u001b[0;32m---> 28\u001b[0m     vec_losses \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([fn(center, mults[i] \u001b[39m*\u001b[39;49m vecs[i]) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(batch_size)])\n\u001b[1;32m     30\u001b[0m     deltas \u001b[39m=\u001b[39m vec_losses \u001b[39m-\u001b[39m center_losses\n\u001b[1;32m     32\u001b[0m     low \u001b[39m=\u001b[39m deltas \u001b[39m<\u001b[39m cutoff\n",
      "File \u001b[0;32m/mnt/ssd-1/adam/basin-volume/src/basin_volume/volume.py:28\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m deltas \u001b[39m=\u001b[39m vec_losses \u001b[39m-\u001b[39m center_losses\n\u001b[1;32m     27\u001b[0m \u001b[39mwhile\u001b[39;00m iters \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39mabs\u001b[39m(deltas \u001b[39m-\u001b[39m cutoff) \u001b[39m>\u001b[39m cutoff \u001b[39m*\u001b[39m rtol):\n\u001b[0;32m---> 28\u001b[0m     vec_losses \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([fn(center, mults[i] \u001b[39m*\u001b[39;49m vecs[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size)])\n\u001b[1;32m     30\u001b[0m     deltas \u001b[39m=\u001b[39m vec_losses \u001b[39m-\u001b[39m center_losses\n\u001b[1;32m     32\u001b[0m     low \u001b[39m=\u001b[39m deltas \u001b[39m<\u001b[39m cutoff\n",
      "File \u001b[0;32m/mnt/ssd-1/adam/basin-volume/src/basin_volume/estimator.py:230\u001b[0m, in \u001b[0;36mCausalLMEstimator.setup_model.<locals>.kl_fn\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mif\u001b[39;00m b:\n\u001b[1;32m    229\u001b[0m     a\u001b[39m.\u001b[39madd_(b)\n\u001b[0;32m--> 230\u001b[0m logits_q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_fn(a, seqs)\n\u001b[1;32m    231\u001b[0m \u001b[39mif\u001b[39;00m b:\n\u001b[1;32m    232\u001b[0m     a\u001b[39m.\u001b[39msub_(b)\n",
      "File \u001b[0;32m/mnt/ssd-1/adam/basin-volume/src/basin_volume/estimator.py:182\u001b[0m, in \u001b[0;36mCausalLMEstimator.setup_model.<locals>.apply_fn\u001b[0;34m(params, x)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mvector_to_parameters(params, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparameters())\n\u001b[0;32m--> 182\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\u001b[39m.\u001b[39mlogits\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/.conda/envs/jax311/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/jax311/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/jax311/lib/python3.11/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:1115\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m \u001b[39m    Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[39m>>> prediction_logits = outputs.logits\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1115\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgpt_neox(\n\u001b[1;32m   1116\u001b[0m     input_ids,\n\u001b[1;32m   1117\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1118\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1119\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1120\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1121\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1122\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1123\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1124\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1125\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1126\u001b[0m     cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m   1127\u001b[0m )\n\u001b[1;32m   1129\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1130\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_out(hidden_states)\n",
      "File \u001b[0;32m~/.conda/envs/jax311/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/jax311/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/jax311/lib/python3.11/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:887\u001b[0m, in \u001b[0;36mGPTNeoXModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    874\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    875\u001b[0m         layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    876\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    884\u001b[0m         position_embeddings,\n\u001b[1;32m    885\u001b[0m     )\n\u001b[1;32m    886\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 887\u001b[0m     outputs \u001b[39m=\u001b[39m layer(\n\u001b[1;32m    888\u001b[0m         hidden_states,\n\u001b[1;32m    889\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mcausal_mask,\n\u001b[1;32m    890\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    891\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    892\u001b[0m         layer_past\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    893\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    894\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    895\u001b[0m         cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m    896\u001b[0m         position_embeddings\u001b[39m=\u001b[39;49mposition_embeddings,\n\u001b[1;32m    897\u001b[0m     )\n\u001b[1;32m    898\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    899\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/jax311/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/jax311/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/jax311/lib/python3.11/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:634\u001b[0m, in \u001b[0;36mGPTNeoXLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, use_cache, layer_past, output_attentions, cache_position, position_embeddings)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mforward\u001b[39m(\n\u001b[1;32m    623\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    624\u001b[0m     hidden_states: Optional[torch\u001b[39m.\u001b[39mFloatTensor],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    632\u001b[0m     position_embeddings: Optional[Tuple[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,  \u001b[39m# necessary, but kept here for BC\u001b[39;00m\n\u001b[1;32m    633\u001b[0m ):\n\u001b[0;32m--> 634\u001b[0m     attention_layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    635\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layernorm(hidden_states),\n\u001b[1;32m    636\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    637\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    638\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    639\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    640\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    641\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    642\u001b[0m         cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m    643\u001b[0m         position_embeddings\u001b[39m=\u001b[39;49mposition_embeddings,\n\u001b[1;32m    644\u001b[0m     )\n\u001b[1;32m    645\u001b[0m     attn_output \u001b[39m=\u001b[39m attention_layer_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: attn_output, present, (attn_weights)\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_dropout(attn_output)\n",
      "File \u001b[0;32m~/.conda/envs/jax311/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/jax311/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/jax311/lib/python3.11/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:321\u001b[0m, in \u001b[0;36mGPTNeoXAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, layer_past, use_cache, output_attentions, padding_mask, cache_position, position_embeddings)\u001b[0m\n\u001b[1;32m    318\u001b[0m bsz, seq_len, _ \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mshape\n\u001b[1;32m    320\u001b[0m \u001b[39m# Apply attention-specific projections and rope\u001b[39;00m\n\u001b[0;32m--> 321\u001b[0m query, key, value, present \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn_projections_and_rope(\n\u001b[1;32m    322\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    323\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    324\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    325\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    326\u001b[0m     cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m    327\u001b[0m     position_embeddings\u001b[39m=\u001b[39;49mposition_embeddings,\n\u001b[1;32m    328\u001b[0m )\n\u001b[1;32m    330\u001b[0m \u001b[39m# Checking for fallbacks in case an unsupported feature is requested\u001b[39;00m\n\u001b[1;32m    331\u001b[0m attention_type \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_attn_implementation\n",
      "File \u001b[0;32m~/.conda/envs/jax311/lib/python3.11/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:435\u001b[0m, in \u001b[0;36mGPTNeoXAttention._attn_projections_and_rope\u001b[0;34m(self, hidden_states, position_ids, layer_past, use_cache, cache_position, position_embeddings)\u001b[0m\n\u001b[1;32m    433\u001b[0m cos, sin \u001b[39m=\u001b[39m position_embeddings\n\u001b[1;32m    434\u001b[0m query, key \u001b[39m=\u001b[39m apply_rotary_pos_emb(query_rot, key_rot, cos, sin)\n\u001b[0;32m--> 435\u001b[0m query \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((query, query_pass), dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    436\u001b[0m key \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((key, key_pass), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    438\u001b[0m \u001b[39m# Cache QKV values\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = estimator.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_largest_tensors():\n",
    "    # Get all tensor objects\n",
    "    tensors = []\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj):\n",
    "                tensors.append(obj)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Group tensors by memory location\n",
    "    memory_dict = {}\n",
    "    for t in tensors:\n",
    "        if t.device.type == 'cuda':\n",
    "            location = t.data_ptr()\n",
    "            if location not in memory_dict:\n",
    "                memory_dict[location] = []\n",
    "            memory_dict[location].append(t)\n",
    "    \n",
    "    # Calculate sizes and sort by memory usage\n",
    "    tensor_sizes = []\n",
    "    for location, tensor_list in memory_dict.items():\n",
    "        # Take the first tensor from each memory location\n",
    "        tensor = tensor_list[0]\n",
    "        size_mb = tensor.nelement() * tensor.element_size() / (1024 * 1024)\n",
    "        tensor_sizes.append((size_mb, tensor.size(), tensor.dtype, len(tensor_list)))\n",
    "    \n",
    "    # Sort by size in descending order\n",
    "    tensor_sizes.sort(reverse=True)\n",
    "    \n",
    "    # Calculate cumulative sizes relative to largest tensor\n",
    "    if tensor_sizes:\n",
    "        largest_size = tensor_sizes[0][0]\n",
    "        cumulative = 0\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{'Size (MB)':>10} {'Cumul.(x)':>10} {'Shape':>20} {'Type':>10} {'Aliases':>8}\")\n",
    "    print(\"-\" * 60)\n",
    "    for size, shape, dtype, num_tensors in tensor_sizes:\n",
    "        cumulative += size\n",
    "        relative_cumul = cumulative / largest_size\n",
    "        print(f\"{size:10.2f} {relative_cumul:10.2f} {str(shape):>20} {str(dtype):>10} {num_tensors:>8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Size (MB)  Cumul.(x)                Shape       Type  Aliases\n",
      "------------------------------------------------------------\n",
      "     53.66       1.00 torch.Size([14067712]) torch.float32        2\n",
      "     24.56       1.46 torch.Size([50304, 128]) torch.float32        1\n",
      "      4.00       1.53 torch.Size([1, 1, 2048, 2048]) torch.bool        1\n",
      "      4.00       1.61 torch.Size([1, 1, 2048, 2048]) torch.bool        1\n",
      "      4.00       1.68 torch.Size([1, 1, 2048, 2048]) torch.bool        1\n",
      "      4.00       1.76 torch.Size([1, 1, 2048, 2048]) torch.bool        1\n",
      "      4.00       1.83 torch.Size([1, 1, 2048, 2048]) torch.bool        1\n",
      "      4.00       1.90 torch.Size([1, 1, 2048, 2048]) torch.bool        1\n",
      "      0.25       1.91 torch.Size([512, 128]) torch.float32        1\n",
      "      0.25       1.91 torch.Size([512, 128]) torch.float32        1\n",
      "      0.25       1.92 torch.Size([512, 128]) torch.float32        1\n",
      "      0.25       1.92 torch.Size([512, 128]) torch.float32        1\n",
      "      0.25       1.93 torch.Size([512, 128]) torch.float32        1\n",
      "      0.25       1.93 torch.Size([512, 128]) torch.float32        1\n",
      "      0.25       1.94 torch.Size([128, 512]) torch.float32        1\n",
      "      0.25       1.94 torch.Size([128, 512]) torch.float32        1\n",
      "      0.25       1.95 torch.Size([128, 512]) torch.float32        1\n",
      "      0.25       1.95 torch.Size([128, 512]) torch.float32        1\n",
      "      0.25       1.96 torch.Size([128, 512]) torch.float32        1\n",
      "      0.25       1.96 torch.Size([128, 512]) torch.float32        1\n",
      "      0.19       1.96 torch.Size([384, 128]) torch.float32        1\n",
      "      0.19       1.97 torch.Size([384, 128]) torch.float32        1\n",
      "      0.19       1.97 torch.Size([384, 128]) torch.float32        1\n",
      "      0.19       1.97 torch.Size([384, 128]) torch.float32        1\n",
      "      0.19       1.98 torch.Size([384, 128]) torch.float32        1\n",
      "      0.19       1.98 torch.Size([384, 128]) torch.float32        1\n",
      "      0.16       1.98 torch.Size([10, 2040]) torch.int64        1\n",
      "      0.06       1.99 torch.Size([128, 128]) torch.float32        1\n",
      "      0.06       1.99 torch.Size([128, 128]) torch.float32        1\n",
      "      0.06       1.99 torch.Size([128, 128]) torch.float32        1\n",
      "      0.06       1.99 torch.Size([128, 128]) torch.float32        1\n",
      "      0.06       1.99 torch.Size([128, 128]) torch.float32        1\n",
      "      0.06       1.99 torch.Size([128, 128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([512]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([512]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([512]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([512]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([512]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([512]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([384]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([384]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([384]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([384]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([384]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([384]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99     torch.Size([10]) torch.float32        1\n",
      "      0.00       1.99     torch.Size([10]) torch.float32        1\n",
      "      0.00       1.99     torch.Size([10]) torch.float32        1\n",
      "      0.00       1.99     torch.Size([10]) torch.float32        1\n",
      "      0.00       1.99     torch.Size([10]) torch.float32        1\n",
      "      0.00       1.99      torch.Size([4]) torch.float32        1\n",
      "      0.00       1.99      torch.Size([4]) torch.float32        1\n",
      "      0.00       1.99      torch.Size([4]) torch.float32        1\n",
      "      0.00       1.99      torch.Size([4]) torch.float32        1\n",
      "      0.00       1.99      torch.Size([4]) torch.float32        1\n",
      "      0.00       1.99      torch.Size([4]) torch.float32        1\n",
      "      0.00       1.99      torch.Size([4]) torch.float32        1\n",
      "      0.00       1.99       torch.Size([]) torch.float32        1\n",
      "      0.00       1.99       torch.Size([]) torch.float32        1\n",
      "      0.00       1.99       torch.Size([]) torch.float32        1\n",
      "      0.00       1.99       torch.Size([]) torch.float32        1\n",
      "      0.00       1.99       torch.Size([]) torch.float32        1\n",
      "      0.00       1.99       torch.Size([]) torch.float32        1\n",
      "      0.00       1.99       torch.Size([]) torch.float32        1\n"
     ]
    }
   ],
   "source": [
    "list_largest_tensors()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
