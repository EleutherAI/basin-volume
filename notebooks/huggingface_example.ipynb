{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/.conda/envs/jax311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "from tyche import VolumeConfig, VolumeEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tyche import ImplicitParamVector, ImplicitRandomVector, ImplicitVector\n",
    "from tyche import print_gpu_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-14m\")\n",
    "model.cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-14m\")\n",
    "tokenizer.pad_token_id = 1  # pythia-specific\n",
    "tokenizer.eos_token_id = 0  # pythia-specific\n",
    "dataset = load_dataset(\"EleutherAI/lambada_openai\", name=\"en\", split=\"test\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens.shape=torch.Size([52947, 8])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg = VolumeConfig(model=model, \n",
    "                   tokenizer=tokenizer, \n",
    "                   dataset=dataset, \n",
    "                   text_key=\"text\",  # must match dataset field\n",
    "                   n_samples=10,  # number of MC samples\n",
    "                   cutoff=1e-2,  # KL-divergence cutoff (nats)\n",
    "                   max_seq_len=8,  # sequence length for chunking dataset\n",
    "                   val_size=10,  # number of sequences or chunks to use in estimation\n",
    "                   data_batch_size=1,\n",
    "                   cache_mode=None,\n",
    "                   chunking=True,\n",
    "                   implicit_vectors=True,\n",
    "                   debug=True,\n",
    "                   )\n",
    "estimator = VolumeEstimator.from_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.config.sigma = tensor(0.3293, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after randn\n",
      "\n",
      "Current GPU memory allocated: 85.80 MB\n",
      "Max GPU memory allocated: 85.80 MB\n",
      "Current GPU memory reserved: 98.00 MB\n",
      "Max GPU memory reserved: 98.00 MB\n",
      "\n",
      "Current GPU memory allocated: 85.80 MB\n",
      "Max GPU memory allocated: 101.80 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:15,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current GPU memory allocated: 85.80 MB\n",
      "Max GPU memory allocated: 101.80 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n",
      "a.shape=torch.Size([])\n",
      "b.shape=torch.Size([1])\n",
      "c.shape=torch.Size([])\n",
      "a = tensor(9.2223, device='cuda:0')\n",
      "b = tensor([0.5342], device='cuda:0')\n",
      "n = 14067711\n",
      "x1 = tensor([0.6953], device='cuda:0')\n",
      "c = tensor(-7033856.5000, device='cuda:0')\n",
      "mu = tensor([0.0579], device='cuda:0')\n",
      "center = tensor([0.0290], device='cuda:0')\n",
      "dist = tensor([1235.0731], device='cuda:0')\n",
      "global_max=tensor([1235.1021], device='cuda:0'), global_in_range=tensor([False], device='cuda:0')\n",
      "\n",
      "Catastrophic cancellation in rad_x1, using linear approximation...\n",
      "\n",
      "f012_int_ln inputs:\n",
      "max_pt = tensor([0.6953], device='cuda:0')\n",
      "x1 = tensor([0.6953], device='cuda:0')\n",
      "f0 = tensor([0.], device='cuda:0')\n",
      "f1 = tensor([20232210.], device='cuda:0')\n",
      "f2 = tensor([-29098032.], device='cuda:0')\n",
      "c = tensor(-7033856.5000, device='cuda:0')\n",
      "\n",
      "f012_int_ln terms:\n",
      "upper = tensor([-16.], device='cuda:0')\n",
      "lower = tensor([-21101584.], device='cuda:0')\n",
      "after randn\n",
      "\n",
      "Current GPU memory allocated: 85.80 MB\n",
      "Max GPU memory allocated: 101.80 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n",
      "\n",
      "Current GPU memory allocated: 85.80 MB\n",
      "Max GPU memory allocated: 101.80 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:03<00:12,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current GPU memory allocated: 85.80 MB\n",
      "Max GPU memory allocated: 101.80 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n",
      "a.shape=torch.Size([])\n",
      "b.shape=torch.Size([1])\n",
      "c.shape=torch.Size([])\n",
      "a = tensor(9.2223, device='cuda:0')\n",
      "b = tensor([5.1240], device='cuda:0')\n",
      "n = 14067711\n",
      "x1 = tensor([0.6289], device='cuda:0')\n",
      "c = tensor(-7033856.5000, device='cuda:0')\n",
      "mu = tensor([0.5556], device='cuda:0')\n",
      "center = tensor([0.2778], device='cuda:0')\n",
      "dist = tensor([1235.0731], device='cuda:0')\n",
      "global_max=tensor([1235.3510], device='cuda:0'), global_in_range=tensor([False], device='cuda:0')\n",
      "\n",
      "Catastrophic cancellation in rad_x1, using linear approximation...\n",
      "\n",
      "f012_int_ln inputs:\n",
      "max_pt = tensor([0.6289], device='cuda:0')\n",
      "x1 = tensor([0.6289], device='cuda:0')\n",
      "f0 = tensor([0.], device='cuda:0')\n",
      "f1 = tensor([22368534.], device='cuda:0')\n",
      "f2 = tensor([-35567364.], device='cuda:0')\n",
      "c = tensor(-7033856.5000, device='cuda:0')\n",
      "\n",
      "f012_int_ln terms:\n",
      "upper = tensor([-15.], device='cuda:0')\n",
      "lower = tensor([-21101580.], device='cuda:0')\n",
      "after randn\n",
      "\n",
      "Current GPU memory allocated: 85.80 MB\n",
      "Max GPU memory allocated: 101.80 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n",
      "\n",
      "Current GPU memory allocated: 85.81 MB\n",
      "Max GPU memory allocated: 101.81 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:04<00:09,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current GPU memory allocated: 85.81 MB\n",
      "Max GPU memory allocated: 101.81 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n",
      "a.shape=torch.Size([])\n",
      "b.shape=torch.Size([1])\n",
      "c.shape=torch.Size([])\n",
      "a = tensor(9.2223, device='cuda:0')\n",
      "b = tensor([5.2002], device='cuda:0')\n",
      "n = 14067711\n",
      "x1 = tensor([0.6758], device='cuda:0')\n",
      "c = tensor(-7033856.5000, device='cuda:0')\n",
      "mu = tensor([0.5639], device='cuda:0')\n",
      "center = tensor([0.2819], device='cuda:0')\n",
      "dist = tensor([1235.0731], device='cuda:0')\n",
      "global_max=tensor([1235.3551], device='cuda:0'), global_in_range=tensor([False], device='cuda:0')\n",
      "\n",
      "Catastrophic cancellation in rad_x1, using linear approximation...\n",
      "\n",
      "f012_int_ln inputs:\n",
      "max_pt = tensor([0.6758], device='cuda:0')\n",
      "x1 = tensor([0.6758], device='cuda:0')\n",
      "f0 = tensor([0.], device='cuda:0')\n",
      "f1 = tensor([20816958.], device='cuda:0')\n",
      "f2 = tensor([-30804296.], device='cuda:0')\n",
      "c = tensor(-7033856.5000, device='cuda:0')\n",
      "\n",
      "f012_int_ln terms:\n",
      "upper = tensor([-17.], device='cuda:0')\n",
      "lower = tensor([-21101586.], device='cuda:0')\n",
      "after randn\n",
      "\n",
      "Current GPU memory allocated: 85.81 MB\n",
      "Max GPU memory allocated: 101.81 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n",
      "\n",
      "Current GPU memory allocated: 85.81 MB\n",
      "Max GPU memory allocated: 101.81 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:05<00:08,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current GPU memory allocated: 85.81 MB\n",
      "Max GPU memory allocated: 101.81 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n",
      "a.shape=torch.Size([])\n",
      "b.shape=torch.Size([1])\n",
      "c.shape=torch.Size([])\n",
      "a = tensor(9.2223, device='cuda:0')\n",
      "b = tensor([-1.8374], device='cuda:0')\n",
      "n = 14067711\n",
      "x1 = tensor([0.6758], device='cuda:0')\n",
      "c = tensor(-7033856.5000, device='cuda:0')\n",
      "mu = tensor([-0.1992], device='cuda:0')\n",
      "center = tensor([-0.0996], device='cuda:0')\n",
      "dist = tensor([1235.0731], device='cuda:0')\n",
      "global_max=tensor([1234.9735], device='cuda:0'), global_in_range=tensor([False], device='cuda:0')\n",
      "\n",
      "Catastrophic cancellation in rad_x1, using linear approximation...\n",
      "\n",
      "f012_int_ln inputs:\n",
      "max_pt = tensor([0.6758], device='cuda:0')\n",
      "x1 = tensor([0.6758], device='cuda:0')\n",
      "f0 = tensor([0.], device='cuda:0')\n",
      "f1 = tensor([20816952.], device='cuda:0')\n",
      "f2 = tensor([-30804304.], device='cuda:0')\n",
      "c = tensor(-7033856.5000, device='cuda:0')\n",
      "\n",
      "f012_int_ln terms:\n",
      "upper = tensor([-16.], device='cuda:0')\n",
      "lower = tensor([-21101582.], device='cuda:0')\n",
      "after randn\n",
      "\n",
      "Current GPU memory allocated: 85.81 MB\n",
      "Max GPU memory allocated: 101.81 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n",
      "\n",
      "Current GPU memory allocated: 85.81 MB\n",
      "Max GPU memory allocated: 101.81 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:06<00:05,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current GPU memory allocated: 85.81 MB\n",
      "Max GPU memory allocated: 101.81 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n",
      "a.shape=torch.Size([])\n",
      "b.shape=torch.Size([1])\n",
      "c.shape=torch.Size([])\n",
      "a = tensor(9.2223, device='cuda:0')\n",
      "b = tensor([0.4572], device='cuda:0')\n",
      "n = 14067711\n",
      "x1 = tensor([0.6875], device='cuda:0')\n",
      "c = tensor(-7033856.5000, device='cuda:0')\n",
      "mu = tensor([0.0496], device='cuda:0')\n",
      "center = tensor([0.0248], device='cuda:0')\n",
      "dist = tensor([1235.0731], device='cuda:0')\n",
      "global_max=tensor([1235.0979], device='cuda:0'), global_in_range=tensor([False], device='cuda:0')\n",
      "\n",
      "Catastrophic cancellation in rad_x1, using linear approximation...\n",
      "\n",
      "f012_int_ln inputs:\n",
      "max_pt = tensor([0.6875], device='cuda:0')\n",
      "x1 = tensor([0.6875], device='cuda:0')\n",
      "f0 = tensor([0.], device='cuda:0')\n",
      "f1 = tensor([20462122.], device='cuda:0')\n",
      "f2 = tensor([-29763106.], device='cuda:0')\n",
      "c = tensor(-7033856.5000, device='cuda:0')\n",
      "\n",
      "f012_int_ln terms:\n",
      "upper = tensor([-15.], device='cuda:0')\n",
      "lower = tensor([-21101584.], device='cuda:0')\n",
      "after randn\n",
      "\n",
      "Current GPU memory allocated: 85.81 MB\n",
      "Max GPU memory allocated: 101.81 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n",
      "\n",
      "Current GPU memory allocated: 85.81 MB\n",
      "Max GPU memory allocated: 101.81 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:07<00:04,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current GPU memory allocated: 85.81 MB\n",
      "Max GPU memory allocated: 101.81 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n",
      "a.shape=torch.Size([])\n",
      "b.shape=torch.Size([1])\n",
      "c.shape=torch.Size([])\n",
      "a = tensor(9.2223, device='cuda:0')\n",
      "b = tensor([2.5348], device='cuda:0')\n",
      "n = 14067711\n",
      "x1 = tensor([0.6602], device='cuda:0')\n",
      "c = tensor(-7033856.5000, device='cuda:0')\n",
      "mu = tensor([0.2749], device='cuda:0')\n",
      "center = tensor([0.1374], device='cuda:0')\n",
      "dist = tensor([1235.0731], device='cuda:0')\n",
      "global_max=tensor([1235.2106], device='cuda:0'), global_in_range=tensor([False], device='cuda:0')\n",
      "\n",
      "Catastrophic cancellation in rad_x1, using linear approximation...\n",
      "\n",
      "f012_int_ln inputs:\n",
      "max_pt = tensor([0.6602], device='cuda:0')\n",
      "x1 = tensor([0.6602], device='cuda:0')\n",
      "f0 = tensor([0.], device='cuda:0')\n",
      "f1 = tensor([21309666.], device='cuda:0')\n",
      "f2 = tensor([-32279746.], device='cuda:0')\n",
      "c = tensor(-7033856.5000, device='cuda:0')\n",
      "\n",
      "f012_int_ln terms:\n",
      "upper = tensor([-18.], device='cuda:0')\n",
      "lower = tensor([-21101588.], device='cuda:0')\n",
      "after randn\n",
      "\n",
      "Current GPU memory allocated: 85.81 MB\n",
      "Max GPU memory allocated: 101.81 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n",
      "\n",
      "Current GPU memory allocated: 85.82 MB\n",
      "Max GPU memory allocated: 101.81 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:08<00:03,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current GPU memory allocated: 85.82 MB\n",
      "Max GPU memory allocated: 101.81 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n",
      "a.shape=torch.Size([])\n",
      "b.shape=torch.Size([1])\n",
      "c.shape=torch.Size([])\n",
      "a = tensor(9.2223, device='cuda:0')\n",
      "b = tensor([1.5928], device='cuda:0')\n",
      "n = 14067711\n",
      "x1 = tensor([0.5625], device='cuda:0')\n",
      "c = tensor(-7033856.5000, device='cuda:0')\n",
      "mu = tensor([0.1727], device='cuda:0')\n",
      "center = tensor([0.0864], device='cuda:0')\n",
      "dist = tensor([1235.0731], device='cuda:0')\n",
      "global_max=tensor([1235.1594], device='cuda:0'), global_in_range=tensor([False], device='cuda:0')\n",
      "\n",
      "Catastrophic cancellation in rad_x1, using linear approximation...\n",
      "\n",
      "f012_int_ln inputs:\n",
      "max_pt = tensor([0.5625], device='cuda:0')\n",
      "x1 = tensor([0.5625], device='cuda:0')\n",
      "f0 = tensor([0.], device='cuda:0')\n",
      "f1 = tensor([25009262.], device='cuda:0')\n",
      "f2 = tensor([-44460928.], device='cuda:0')\n",
      "c = tensor(-7033856.5000, device='cuda:0')\n",
      "\n",
      "f012_int_ln terms:\n",
      "upper = tensor([-15.5000], device='cuda:0')\n",
      "lower = tensor([-21101582.], device='cuda:0')\n",
      "after randn\n",
      "\n",
      "Current GPU memory allocated: 85.82 MB\n",
      "Max GPU memory allocated: 101.81 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n",
      "\n",
      "Current GPU memory allocated: 85.82 MB\n",
      "Max GPU memory allocated: 101.82 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:09<00:02,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current GPU memory allocated: 85.82 MB\n",
      "Max GPU memory allocated: 101.82 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n",
      "a.shape=torch.Size([])\n",
      "b.shape=torch.Size([1])\n",
      "c.shape=torch.Size([])\n",
      "a = tensor(9.2223, device='cuda:0')\n",
      "b = tensor([0.4427], device='cuda:0')\n",
      "n = 14067711\n",
      "x1 = tensor([0.7227], device='cuda:0')\n",
      "c = tensor(-7033856.5000, device='cuda:0')\n",
      "mu = tensor([0.0480], device='cuda:0')\n",
      "center = tensor([0.0240], device='cuda:0')\n",
      "dist = tensor([1235.0731], device='cuda:0')\n",
      "global_max=tensor([1235.0972], device='cuda:0'), global_in_range=tensor([False], device='cuda:0')\n",
      "\n",
      "Catastrophic cancellation in rad_x1, using linear approximation...\n",
      "\n",
      "f012_int_ln inputs:\n",
      "max_pt = tensor([0.7227], device='cuda:0')\n",
      "x1 = tensor([0.7227], device='cuda:0')\n",
      "f0 = tensor([0.], device='cuda:0')\n",
      "f1 = tensor([19466664.], device='cuda:0')\n",
      "f2 = tensor([-26937672.], device='cuda:0')\n",
      "c = tensor(-7033856.5000, device='cuda:0')\n",
      "\n",
      "f012_int_ln terms:\n",
      "upper = tensor([-16.5000], device='cuda:0')\n",
      "lower = tensor([-21101580.], device='cuda:0')\n",
      "after randn\n",
      "\n",
      "Current GPU memory allocated: 85.82 MB\n",
      "Max GPU memory allocated: 101.82 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n",
      "\n",
      "Current GPU memory allocated: 85.82 MB\n",
      "Max GPU memory allocated: 101.82 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:11<00:01,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current GPU memory allocated: 85.82 MB\n",
      "Max GPU memory allocated: 101.82 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n",
      "a.shape=torch.Size([])\n",
      "b.shape=torch.Size([1])\n",
      "c.shape=torch.Size([])\n",
      "a = tensor(9.2223, device='cuda:0')\n",
      "b = tensor([2.4128], device='cuda:0')\n",
      "n = 14067711\n",
      "x1 = tensor([0.5430], device='cuda:0')\n",
      "c = tensor(-7033856.5000, device='cuda:0')\n",
      "mu = tensor([0.2616], device='cuda:0')\n",
      "center = tensor([0.1308], device='cuda:0')\n",
      "dist = tensor([1235.0731], device='cuda:0')\n",
      "global_max=tensor([1235.2040], device='cuda:0'), global_in_range=tensor([False], device='cuda:0')\n",
      "\n",
      "Catastrophic cancellation in rad_x1, using linear approximation...\n",
      "\n",
      "f012_int_ln inputs:\n",
      "max_pt = tensor([0.5430], device='cuda:0')\n",
      "x1 = tensor([0.5430], device='cuda:0')\n",
      "f0 = tensor([0.], device='cuda:0')\n",
      "f1 = tensor([25908876.], device='cuda:0')\n",
      "f2 = tensor([-47717080.], device='cuda:0')\n",
      "c = tensor(-7033856.5000, device='cuda:0')\n",
      "\n",
      "f012_int_ln terms:\n",
      "upper = tensor([-15.], device='cuda:0')\n",
      "lower = tensor([-21101576.], device='cuda:0')\n",
      "after randn\n",
      "\n",
      "Current GPU memory allocated: 85.82 MB\n",
      "Max GPU memory allocated: 101.82 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n",
      "\n",
      "Current GPU memory allocated: 85.82 MB\n",
      "Max GPU memory allocated: 101.82 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:12<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current GPU memory allocated: 85.82 MB\n",
      "Max GPU memory allocated: 101.82 MB\n",
      "Current GPU memory reserved: 118.00 MB\n",
      "Max GPU memory reserved: 118.00 MB\n",
      "a.shape=torch.Size([])\n",
      "b.shape=torch.Size([1])\n",
      "c.shape=torch.Size([])\n",
      "a = tensor(9.2223, device='cuda:0')\n",
      "b = tensor([-0.2316], device='cuda:0')\n",
      "n = 14067711\n",
      "x1 = tensor([0.6562], device='cuda:0')\n",
      "c = tensor(-7033856.5000, device='cuda:0')\n",
      "mu = tensor([-0.0251], device='cuda:0')\n",
      "center = tensor([-0.0126], device='cuda:0')\n",
      "dist = tensor([1235.0731], device='cuda:0')\n",
      "global_max=tensor([1235.0605], device='cuda:0'), global_in_range=tensor([False], device='cuda:0')\n",
      "\n",
      "Catastrophic cancellation in rad_x1, using linear approximation...\n",
      "\n",
      "f012_int_ln inputs:\n",
      "max_pt = tensor([0.6562], device='cuda:0')\n",
      "x1 = tensor([0.6562], device='cuda:0')\n",
      "f0 = tensor([0.], device='cuda:0')\n",
      "f1 = tensor([21436506.], device='cuda:0')\n",
      "f2 = tensor([-32665172.], device='cuda:0')\n",
      "c = tensor(-7033856.5000, device='cuda:0')\n",
      "\n",
      "f012_int_ln terms:\n",
      "upper = tensor([-16.], device='cuda:0')\n",
      "lower = tensor([-21101580.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = estimator.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_largest_tensors():\n",
    "    # Get all tensor objects\n",
    "    tensors = []\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj):\n",
    "                tensors.append(obj)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Group tensors by memory location\n",
    "    memory_dict = {}\n",
    "    for t in tensors:\n",
    "        if t.device.type == 'cuda':\n",
    "            location = t.data_ptr()\n",
    "            if location not in memory_dict:\n",
    "                memory_dict[location] = []\n",
    "            memory_dict[location].append(t)\n",
    "    \n",
    "    # Calculate sizes and sort by memory usage\n",
    "    tensor_sizes = []\n",
    "    for location, tensor_list in memory_dict.items():\n",
    "        # Take the first tensor from each memory location\n",
    "        tensor = tensor_list[0]\n",
    "        size_mb = tensor.nelement() * tensor.element_size() / (1024 * 1024)\n",
    "        tensor_sizes.append((size_mb, tensor.size(), tensor.dtype, len(tensor_list)))\n",
    "    \n",
    "    # Sort by size in descending order\n",
    "    tensor_sizes.sort(reverse=True)\n",
    "    \n",
    "    # Calculate cumulative sizes relative to largest tensor\n",
    "    if tensor_sizes:\n",
    "        largest_size = tensor_sizes[0][0]\n",
    "        cumulative = 0\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{'Size (MB)':>10} {'Cumul.(x)':>10} {'Shape':>20} {'Type':>10} {'Aliases':>8}\")\n",
    "    print(\"-\" * 60)\n",
    "    for size, shape, dtype, num_tensors in tensor_sizes:\n",
    "        cumulative += size\n",
    "        relative_cumul = cumulative / largest_size\n",
    "        print(f\"{size:10.2f} {relative_cumul:10.2f} {str(shape):>20} {str(dtype):>10} {num_tensors:>8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Size (MB)  Cumul.(x)                Shape       Type  Aliases\n",
      "------------------------------------------------------------\n",
      "     53.66       1.00 torch.Size([14067712]) torch.float32        2\n",
      "     24.56       1.46 torch.Size([50304, 128]) torch.float32        1\n",
      "      4.00       1.53 torch.Size([1, 1, 2048, 2048]) torch.bool        1\n",
      "      4.00       1.61 torch.Size([1, 1, 2048, 2048]) torch.bool        1\n",
      "      4.00       1.68 torch.Size([1, 1, 2048, 2048]) torch.bool        1\n",
      "      4.00       1.76 torch.Size([1, 1, 2048, 2048]) torch.bool        1\n",
      "      4.00       1.83 torch.Size([1, 1, 2048, 2048]) torch.bool        1\n",
      "      4.00       1.90 torch.Size([1, 1, 2048, 2048]) torch.bool        1\n",
      "      0.25       1.91 torch.Size([512, 128]) torch.float32        1\n",
      "      0.25       1.91 torch.Size([512, 128]) torch.float32        1\n",
      "      0.25       1.92 torch.Size([512, 128]) torch.float32        1\n",
      "      0.25       1.92 torch.Size([512, 128]) torch.float32        1\n",
      "      0.25       1.93 torch.Size([512, 128]) torch.float32        1\n",
      "      0.25       1.93 torch.Size([512, 128]) torch.float32        1\n",
      "      0.25       1.94 torch.Size([128, 512]) torch.float32        1\n",
      "      0.25       1.94 torch.Size([128, 512]) torch.float32        1\n",
      "      0.25       1.95 torch.Size([128, 512]) torch.float32        1\n",
      "      0.25       1.95 torch.Size([128, 512]) torch.float32        1\n",
      "      0.25       1.96 torch.Size([128, 512]) torch.float32        1\n",
      "      0.25       1.96 torch.Size([128, 512]) torch.float32        1\n",
      "      0.19       1.96 torch.Size([384, 128]) torch.float32        1\n",
      "      0.19       1.97 torch.Size([384, 128]) torch.float32        1\n",
      "      0.19       1.97 torch.Size([384, 128]) torch.float32        1\n",
      "      0.19       1.97 torch.Size([384, 128]) torch.float32        1\n",
      "      0.19       1.98 torch.Size([384, 128]) torch.float32        1\n",
      "      0.19       1.98 torch.Size([384, 128]) torch.float32        1\n",
      "      0.16       1.98 torch.Size([10, 2040]) torch.int64        1\n",
      "      0.06       1.99 torch.Size([128, 128]) torch.float32        1\n",
      "      0.06       1.99 torch.Size([128, 128]) torch.float32        1\n",
      "      0.06       1.99 torch.Size([128, 128]) torch.float32        1\n",
      "      0.06       1.99 torch.Size([128, 128]) torch.float32        1\n",
      "      0.06       1.99 torch.Size([128, 128]) torch.float32        1\n",
      "      0.06       1.99 torch.Size([128, 128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([512]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([512]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([512]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([512]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([512]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([512]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([384]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([384]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([384]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([384]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([384]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([384]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99    torch.Size([128]) torch.float32        1\n",
      "      0.00       1.99     torch.Size([10]) torch.float32        1\n",
      "      0.00       1.99     torch.Size([10]) torch.float32        1\n",
      "      0.00       1.99     torch.Size([10]) torch.float32        1\n",
      "      0.00       1.99     torch.Size([10]) torch.float32        1\n",
      "      0.00       1.99     torch.Size([10]) torch.float32        1\n",
      "      0.00       1.99      torch.Size([4]) torch.float32        1\n",
      "      0.00       1.99      torch.Size([4]) torch.float32        1\n",
      "      0.00       1.99      torch.Size([4]) torch.float32        1\n",
      "      0.00       1.99      torch.Size([4]) torch.float32        1\n",
      "      0.00       1.99      torch.Size([4]) torch.float32        1\n",
      "      0.00       1.99      torch.Size([4]) torch.float32        1\n",
      "      0.00       1.99      torch.Size([4]) torch.float32        1\n",
      "      0.00       1.99       torch.Size([]) torch.float32        1\n",
      "      0.00       1.99       torch.Size([]) torch.float32        1\n",
      "      0.00       1.99       torch.Size([]) torch.float32        1\n",
      "      0.00       1.99       torch.Size([]) torch.float32        1\n",
      "      0.00       1.99       torch.Size([]) torch.float32        1\n",
      "      0.00       1.99       torch.Size([]) torch.float32        1\n",
      "      0.00       1.99       torch.Size([]) torch.float32        1\n"
     ]
    }
   ],
   "source": [
    "list_largest_tensors()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
