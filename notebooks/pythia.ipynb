{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "from basin_volume import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kl_fn(probs_p, apply_fn, x, l2_reg):\n",
    "    def kl_fn(a, b):\n",
    "        params_q = a + b\n",
    "        logits_q = apply_fn(params_q, x)\n",
    "        logprobs_q = jax.nn.log_softmax(logits_q)\n",
    "        kl_term = optax.kl_divergence(logprobs_q, probs_p).mean()\n",
    "        l2_term = 1/2 * l2_reg * jnp.sum(b**2)\n",
    "        return kl_term + l2_term\n",
    "    return kl_fn\n",
    "\n",
    "def make_kl_fn_params(params_p, apply_fn, x, *, l2_reg):\n",
    "    logits_p = apply_fn(params_p, x)\n",
    "    probs_p = jax.nn.softmax(logits_p)\n",
    "    return make_kl_fn(probs_p, apply_fn, x, l2_reg=l2_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-14m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50277"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-14m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 14067712\n",
      "Total number of embedding parameters: 12877824\n",
      "Total number of non-embedding parameters: 1189888\n",
      "45.77% torch.Size([50304, 128]) embed_out.weight\n",
      "45.77% torch.Size([50304, 128]) gpt_neox.embed_in.weight\n",
      "0.47% torch.Size([128, 512]) gpt_neox.layers.5.mlp.dense_4h_to_h.weight\n",
      "0.47% torch.Size([512, 128]) gpt_neox.layers.5.mlp.dense_h_to_4h.weight\n",
      "0.47% torch.Size([128, 512]) gpt_neox.layers.4.mlp.dense_4h_to_h.weight\n",
      "0.47% torch.Size([512, 128]) gpt_neox.layers.4.mlp.dense_h_to_4h.weight\n",
      "0.47% torch.Size([128, 512]) gpt_neox.layers.3.mlp.dense_4h_to_h.weight\n",
      "0.47% torch.Size([512, 128]) gpt_neox.layers.3.mlp.dense_h_to_4h.weight\n",
      "0.47% torch.Size([128, 512]) gpt_neox.layers.2.mlp.dense_4h_to_h.weight\n",
      "0.47% torch.Size([512, 128]) gpt_neox.layers.2.mlp.dense_h_to_4h.weight\n",
      "0.47% torch.Size([128, 512]) gpt_neox.layers.1.mlp.dense_4h_to_h.weight\n",
      "0.47% torch.Size([512, 128]) gpt_neox.layers.1.mlp.dense_h_to_4h.weight\n",
      "0.47% torch.Size([128, 512]) gpt_neox.layers.0.mlp.dense_4h_to_h.weight\n",
      "0.47% torch.Size([512, 128]) gpt_neox.layers.0.mlp.dense_h_to_4h.weight\n",
      "0.35% torch.Size([384, 128]) gpt_neox.layers.5.attention.query_key_value.weight\n",
      "0.35% torch.Size([384, 128]) gpt_neox.layers.4.attention.query_key_value.weight\n",
      "0.35% torch.Size([384, 128]) gpt_neox.layers.3.attention.query_key_value.weight\n",
      "0.35% torch.Size([384, 128]) gpt_neox.layers.2.attention.query_key_value.weight\n",
      "0.35% torch.Size([384, 128]) gpt_neox.layers.1.attention.query_key_value.weight\n",
      "0.35% torch.Size([384, 128]) gpt_neox.layers.0.attention.query_key_value.weight\n",
      "0.12% torch.Size([128, 128]) gpt_neox.layers.5.attention.dense.weight\n",
      "0.12% torch.Size([128, 128]) gpt_neox.layers.4.attention.dense.weight\n",
      "0.12% torch.Size([128, 128]) gpt_neox.layers.3.attention.dense.weight\n",
      "0.12% torch.Size([128, 128]) gpt_neox.layers.2.attention.dense.weight\n",
      "0.12% torch.Size([128, 128]) gpt_neox.layers.1.attention.dense.weight\n",
      "0.12% torch.Size([128, 128]) gpt_neox.layers.0.attention.dense.weight\n",
      "0.00% torch.Size([512]) gpt_neox.layers.5.mlp.dense_h_to_4h.bias\n",
      "0.00% torch.Size([512]) gpt_neox.layers.4.mlp.dense_h_to_4h.bias\n",
      "0.00% torch.Size([512]) gpt_neox.layers.3.mlp.dense_h_to_4h.bias\n",
      "0.00% torch.Size([512]) gpt_neox.layers.2.mlp.dense_h_to_4h.bias\n",
      "0.00% torch.Size([512]) gpt_neox.layers.1.mlp.dense_h_to_4h.bias\n",
      "0.00% torch.Size([512]) gpt_neox.layers.0.mlp.dense_h_to_4h.bias\n",
      "0.00% torch.Size([384]) gpt_neox.layers.5.attention.query_key_value.bias\n",
      "0.00% torch.Size([384]) gpt_neox.layers.4.attention.query_key_value.bias\n",
      "0.00% torch.Size([384]) gpt_neox.layers.3.attention.query_key_value.bias\n",
      "0.00% torch.Size([384]) gpt_neox.layers.2.attention.query_key_value.bias\n",
      "0.00% torch.Size([384]) gpt_neox.layers.1.attention.query_key_value.bias\n",
      "0.00% torch.Size([384]) gpt_neox.layers.0.attention.query_key_value.bias\n",
      "0.00% torch.Size([128]) gpt_neox.final_layer_norm.bias\n",
      "0.00% torch.Size([128]) gpt_neox.final_layer_norm.weight\n",
      "0.00% torch.Size([128]) gpt_neox.layers.5.mlp.dense_4h_to_h.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.5.attention.dense.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.5.post_attention_layernorm.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.5.post_attention_layernorm.weight\n",
      "0.00% torch.Size([128]) gpt_neox.layers.5.input_layernorm.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.5.input_layernorm.weight\n",
      "0.00% torch.Size([128]) gpt_neox.layers.4.mlp.dense_4h_to_h.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.4.attention.dense.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.4.post_attention_layernorm.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.4.post_attention_layernorm.weight\n",
      "0.00% torch.Size([128]) gpt_neox.layers.4.input_layernorm.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.4.input_layernorm.weight\n",
      "0.00% torch.Size([128]) gpt_neox.layers.3.mlp.dense_4h_to_h.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.3.attention.dense.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.3.post_attention_layernorm.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.3.post_attention_layernorm.weight\n",
      "0.00% torch.Size([128]) gpt_neox.layers.3.input_layernorm.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.3.input_layernorm.weight\n",
      "0.00% torch.Size([128]) gpt_neox.layers.2.mlp.dense_4h_to_h.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.2.attention.dense.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.2.post_attention_layernorm.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.2.post_attention_layernorm.weight\n",
      "0.00% torch.Size([128]) gpt_neox.layers.2.input_layernorm.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.2.input_layernorm.weight\n",
      "0.00% torch.Size([128]) gpt_neox.layers.1.mlp.dense_4h_to_h.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.1.attention.dense.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.1.post_attention_layernorm.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.1.post_attention_layernorm.weight\n",
      "0.00% torch.Size([128]) gpt_neox.layers.1.input_layernorm.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.1.input_layernorm.weight\n",
      "0.00% torch.Size([128]) gpt_neox.layers.0.mlp.dense_4h_to_h.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.0.attention.dense.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.0.post_attention_layernorm.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.0.post_attention_layernorm.weight\n",
      "0.00% torch.Size([128]) gpt_neox.layers.0.input_layernorm.bias\n",
      "0.00% torch.Size([128]) gpt_neox.layers.0.input_layernorm.weight\n"
     ]
    }
   ],
   "source": [
    "# for name, p in model.named_parameters():\n",
    "#     print(name, p.shape)\n",
    "# print()\n",
    "\n",
    "# sort by number of parameters and print\n",
    "sorted_params = sorted(model.named_parameters(), key=lambda x: np.prod(x[1].shape))\n",
    "# compute total number of parameters\n",
    "total_params = sum(np.prod(p.shape) for name, p in sorted_params)\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "\n",
    "# compute total embedding parameters\n",
    "embedding_params = sum(np.prod(p.shape) for name, p in sorted_params if \"embed\" in name)\n",
    "print(f\"Total number of embedding parameters: {embedding_params}\")\n",
    "print(f\"Total number of non-embedding parameters: {total_params - embedding_params}\")\n",
    "\n",
    "for name, p in reversed(sorted_params):\n",
    "    print(f\"{np.prod(p.shape) / total_params:.2%}\", p.shape, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to convert Torch params to flat array and back\n",
    "def torch_to_flat(model):\n",
    "    \"\"\"Convert PyTorch model parameters to flat array\"\"\"\n",
    "    return np.concatenate([p.detach().numpy().ravel() for p in model.parameters()])\n",
    "\n",
    "def flat_to_torch(flat_params, model):\n",
    "    \"\"\"Convert flat array back to PyTorch model parameters\"\"\"\n",
    "    pointer = 0\n",
    "    for param in model.parameters():\n",
    "        num_params = param.numel()\n",
    "        param.data = torch.from_numpy(\n",
    "            flat_params[pointer:pointer + num_params].reshape(param.shape)\n",
    "        )\n",
    "        pointer += num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_params = torch_to_flat(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mX_train\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/EleutherAI/pythia/blob/main/models/14M/pythia-14m.yml\n",
    "with open(\"../data/pythia-14m.yml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "l2_reg = config['weight-decay']\n",
    "# TODO: determine if this is the correct train size\n",
    "train_size = config['train-iters'] * config['train_micro_batch_size_per_gpu']\n",
    "sigma_epoch = 1/jnp.sqrt(l2_reg * train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_params = jnp.sqrt(jnp.mean(params**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001478281\n",
      "0.3292918\n"
     ]
    }
   ],
   "source": [
    "print(sigma_epoch)  # way too small!\n",
    "print(sigma_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fn(params, x):\n",
    "    # assign params to model\n",
    "    flat_to_torch(params, model)\n",
    "    return model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_fn = make_kl_fn_params(trained_params, apply_fn, X_train, l2_reg=0.)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF = 1e-2\n",
    "\n",
    "RESULTS = {} # estimates, props, mults, deltas, logabsint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS['naive'] = get_estimates_vectorized_gauss(1, \n",
    "                                                  sigma=sigma_params,\n",
    "                                                  fn=kl_fn, \n",
    "                                                  params=trained_params, \n",
    "                                                  cutoff=CUTOFF,\n",
    "                                                  tol=0.1,\n",
    "                                                  debug=False,\n",
    "                                                 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
